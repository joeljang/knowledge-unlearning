{
    "mode": "general_lm_eval",
    "wandb_project": "Knowledge Unlearning",
    "wandb_run_name": "example",
    "num_train_epochs": 20,
    "check_val_every_n_epoch": 1,
    "check_validation_only": true,
    "do_init_eval": true,
    "train_set": "data/main/lm_extraction_32_0.csv",
    "valid_sets": [
        "validation_data/wizard_of_wikipedia.json",
        "validation_data/empathetic_dialogues.json",
        "validation_data/blended_skill_talk.json",
        "validation_data/wizard_of_internet.json"
    ],
    "valid_subset_path": [
        "",
        "",
        "",
        ""
    ],
    "valid_type_path": [
        "",
        "",
        "",
        ""
    ],
    "train_batch_size": 8,
    "eval_batch_size": 16,
    "gradient_accumulation_steps": 1,
    "ngpu": 1,
    "learning_rate": 5e-5,
    "model_name_or_path": "EleutherAI/gpt-neo-125M",
    "el_threshold": 0.0499,
    "ma_threshold": 0.2994,
    "input_length": 512,
    "output_length": 512,
    "target_length": 200,
    "num_workers": 64,
    "strategy": "deepspeed_stage_2_offload",
    "fp16": true,
    "wandb_log": true
}
